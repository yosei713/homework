{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84f1e8f",
   "metadata": {},
   "source": [
    "# SGD\n",
    "数式では\n",
    "$$W←W-\\eta\\frac{\\partial L}{\\partial W}$$のように記される。\n",
    "SGDは勾配方向へ一定の距離だけ進める単純な方法。  \n",
    "以下が実装である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ba50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "        def update(self, param, grads):\n",
    "            for key in param.key():\n",
    "                param[key] -= self.lr * grads[key] #lrはlearning rate(学習係数)を示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5768b4",
   "metadata": {},
   "source": [
    "params,gradsはディクショナリ変数。\n",
    "## SGDの欠点\n",
    "関数の形状が伸びた形のものであると、非効率の経路で探索する点。\n",
    "# momentum \n",
    "$$v←\\alpha v - \\eta\\frac{\\partial L}{\\partial W}$$\n",
    "$$W←W-v$$\n",
    "この式は物理に関係したものである。上の式は物体が勾配方向に力を受けることを示す。αは物体が何も力を受けないときに徐々に減速する役割を担う。  \n",
    "以下が実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd960a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "\n",
    "    \"\"\"Momentum SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390a549",
   "metadata": {},
   "source": [
    "これで問題を解くと、ｘ軸方向に受ける力は小さい一方で常に同方向への力を受けるため同方向に一定して加速できる。  \n",
    "y軸は正と負の力を交互に受けるためお互いに打ち消しあってｙ軸方向の力が安定しない。\n",
    "# AdaGram\n",
    "ニューラルネットワークにおいては学習係数が大きすぎても小さすぎてもよくない。  \n",
    "これを効率的に解決するためには学習係数の減衰というものがある。これは学習が進むにつれて学習係数を小さくするといったものだ。  \n",
    "$$h←h +\\frac{\\partial L}{\\partial W}・\\frac{\\partial L}{\\partial W}$$\n",
    "$$W←W - \\eta\\frac{\\partial 1}{\\partial \\sqrt{h}}\\frac{\\partial L}{\\partial W}$$\n",
    "hの二乗根分の1を乗算することで学習のスケールを調整する。  \n",
    "つまり、よく動いたパラメーターの学習係数が次第に小さくなるという学習係数の減衰をパラメーターの要素ごとに行うのだ。  \n",
    "以下が実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ba2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcce40",
   "metadata": {},
   "source": [
    "# Adam\n",
    "momentumではボールがお椀を転がるように物理法則に準じた動きをした。  \n",
    "AdamGradではパラメーターの要素ごとに適応的に更新ステップを調整した。  \n",
    "これら二つを融合させたのがAdamである。  \n",
    "この二つが融合することで効率的にパラメーター空間を探索することが期待できる。  \n",
    "また、ハイパーパラメータの倍悪補正も行われている。\n",
    "# どの更新手段を使うか\n",
    "残念なことにすべてにおいて優れていｒ手法はないため、それぞれの特徴に合わせて使うことが求められる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f00a66",
   "metadata": {},
   "source": [
    "# 重みの初期値\n",
    "重みを小さくすることで過学習は起きにくくなる。  \n",
    "一方で負の数出ない数で最も小さい数の0を重みに設定すると正しい学習が行えない。  \n",
    "正確には均一の値に重みを設定してはならないのだ。  \n",
    "なぜなら誤差逆伝播法で均一に重みが更新されてしまうからだ。  \n",
    "すると重みが対照的な値(重複した値)を持ってしまい、たくさんの重みをもつ必要がなくなってしまうからだ。<br><br>\n",
    "隠れ層のアクティベーションは難しくて理解ができなかった。  \n",
    "結論としては、活性化関数にReLUを使うときはHeの初期値を、sigmoidやtanhなどのS字カーブを使うときはXavierの初期値を使うというのが現段階のベストプラクティスである。\n",
    "# Batch Normalization\n",
    "強制的にアクティベーションの分布を調整するのがBatchNormalizationである。  \n",
    "これの利点は\n",
    "・学習を早く進行できる(学習係数を大きくできる)  \n",
    "・初期値にそれほど依存しない、初期値に神経質にならなくてよい  \n",
    "・過学習を抑制する。\n",
    "# 正則化\n",
    "機械学習においては過学習が問題となることがよくある  \n",
    "その理由は大きく二つあげられる。  \n",
    "・パラメータを大量に持ち表現性の高いモデルであること。  \n",
    "・訓練データが少ないこと。\n",
    "# Weight decay\n",
    "Weight decay は過学習抑制によく使われる。  \n",
    "これは学習の過程において大きな重みをもつことにペナルティ課すことで過学習を抑制しようというものである。\n",
    "# Dropout \n",
    "ニューラルネットワークが複雑化するとWeight decayのみでは対応が困難になる。  \n",
    "そこでDropoutという手法が用いられる。訓練時に隠れ層のニューロンをランダムに消去して学習する手段だ。  \n",
    "消去されたニューロンは信号の伝達が行われなくなる。\n",
    "# ハイパーパラメータの検証\n",
    "ニューラルネットワークでは重みやバイアスとは別にハイパーパラメータが多く登場する。  \n",
    "ここでいうハイパーパラメータとは各層のニューロン数やバッチサイズ、パラメータの更新の際の学習係数やWeight decay などである。\n",
    "# 検証データ\n",
    "訓練データ、テストデータとは別に検証データも必要である。なぜならば汎化性能を調べるためにテストデータを用いても、テストデータに対して過度に適応してしまうことがあるからだ。検証データはハイパーパラメータの調整のために用いる。  \n",
    "テストデータは汎化性能の確認のため、一度だけ用いるのが理想的である。　\n",
    "# ハイパーパラメータの最適化\n",
    "ディープラーニングの学習には多くの時間を要するため、筋の悪そうなハイパーパラメータは早い段階で捨てることが重要である。  \n",
    "ハイパーパラメータの範囲はおおまかにざっくりと設定するのが有効である。ざっくりと指定するとは10<sup>-3</sup>から10<sup>3</sup>のようの10のn乗のスケールで指定することだ。  \n",
    "やることをまとめると  \n",
    "ステップ１　ハイパーパラメータの範囲を指定する。  \n",
    "ステップ２　指定されたハイパーパラメータの範囲からランダムにサンプリングする。\n",
    "ステップ３　ステップ１でサンプリングされたハイパーパラメータの値を使用して学習を行い、検証データで認識制度を評価する。　\n",
    "ステップ４　ステップ２，３を100回程度繰り返してそれら認識精度の結果からハイパーパラメータの範囲を狭める。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
